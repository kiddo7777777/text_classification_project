{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9daedb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "095e4a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  \n",
    "    # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                                        vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77ca281",
   "metadata": {},
   "source": [
    "## data read in and simple cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1623478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.read_csv('SBF_trn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7da4ebf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = df_trn[['post','annotatorGender','annotatorRace','annotatorAge','offensiveYN']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "87cbce3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112900, 5)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf474228",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = '^RT.*: '\n",
    "pattern_2 ='&#[^a-zA-Z]+;$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5f3ec17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full['clean_post']=[re.sub(pattern_2,'',re.sub(pattern,'',x)) for x in df_full['post']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c3df612",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = df_full[df_full['offensiveYN'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b133e108",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full['label']= [x if x!=0.5 else 3 for x in df_full['offensiveYN']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cab74793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110883, 7)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "798c93a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_agg = df_full.groupby(by=[\"clean_post\",'annotatorGender','annotatorRace','annotatorAge'])['offensiveYN'].agg(lambda x:pd.Series.mode(x)[0]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "12e136ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88465, 5)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45aa64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94891ee6",
   "metadata": {},
   "source": [
    "### Model with smaller data set to compare with baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d61c33e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#35419\n",
    "sample_df = df_full_agg.sample(n=35419)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "adccd44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35419, 5)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c4bd24ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embedding_sample  = pd.get_dummies(sample_df, columns = ['annotatorGender','annotatorRace'])\n",
    "\n",
    "train,test = train_test_split(df_embedding_sample, test_size=0.2, random_state=42, shuffle=True)\n",
    "y_train = to_categorical(train['offensiveYN'].values, 3)\n",
    "y_test = to_categorical(test['offensiveYN'].values, 3)\n",
    "\n",
    "text_train = train['clean_post'].values\n",
    "num_train = train['annotatorAge'].values\n",
    "cat_train = train.iloc[:,3:].to_numpy()\n",
    "\n",
    "text_test = test['clean_post'].values\n",
    "num_test = test['annotatorAge'].values\n",
    "cat_test = test.iloc[:,3:].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8afaf4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000) ### change \n",
    "tokenizer.fit_on_texts(text_train)\n",
    "\n",
    "X_train_text = tokenizer.texts_to_sequences(text_train)\n",
    "X_test_text = tokenizer.texts_to_sequences(text_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "27c5da33",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text = pad_sequences(X_train_text, padding='post', maxlen=maxlen)\n",
    "X_test_text = pad_sequences(X_test_text, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "10bea491",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "embedding_matrix = create_embedding_matrix('glove.6B.50d.txt' ,\n",
    "                                            tokenizer.word_index,  \n",
    "                                            embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6393c986",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_embedding = min(np.ceil((cat_train.shape[1])/2), 50 ) ## determine categorical embedding size using conventional method\n",
    "cat_embedding_size = int(cat_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997869bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "74ea5166",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "5667/5667 [==============================] - 179s 31ms/step - loss: 0.5404 - accuracy: 0.7350 - val_loss: 0.4893 - val_accuracy: 0.7668\n",
      "Epoch 2/7\n",
      "5667/5667 [==============================] - 186s 33ms/step - loss: 0.4297 - accuracy: 0.8066 - val_loss: 0.4533 - val_accuracy: 0.7887\n",
      "Epoch 3/7\n",
      "5667/5667 [==============================] - 212s 37ms/step - loss: 0.3522 - accuracy: 0.8481 - val_loss: 0.4732 - val_accuracy: 0.7842\n",
      "Epoch 4/7\n",
      "5667/5667 [==============================] - 218s 38ms/step - loss: 0.2899 - accuracy: 0.8791 - val_loss: 0.4907 - val_accuracy: 0.7846\n",
      "Epoch 5/7\n",
      "5667/5667 [==============================] - 220s 39ms/step - loss: 0.2451 - accuracy: 0.9019 - val_loss: 0.5852 - val_accuracy: 0.7815\n",
      "Epoch 6/7\n",
      "5667/5667 [==============================] - 210s 37ms/step - loss: 0.2102 - accuracy: 0.9155 - val_loss: 0.6127 - val_accuracy: 0.7740\n",
      "Epoch 7/7\n",
      "5667/5667 [==============================] - 203s 36ms/step - loss: 0.1875 - accuracy: 0.9241 - val_loss: 0.6482 - val_accuracy: 0.7787\n"
     ]
    }
   ],
   "source": [
    "inp_text_data = keras.layers.Input(shape=(X_train_text.shape[1],))\n",
    "inp_cat_data = keras.layers.Input(shape=(cat_train.shape[1],))\n",
    "inp_num_data = keras.layers.Input(shape=(1,))\n",
    "emb= keras.layers.Embedding(vocab_size, embedding_dim, input_length=maxlen)(inp_text_data)\n",
    "emb_2 = keras.layers.Embedding(input_dim=cat_train.shape[1], output_dim=cat_embedding_size)(inp_cat_data)\n",
    "conv_1 = keras.layers.Conv1D(128, 5, activation='relu')(emb)\n",
    "conv_2 = keras.layers.Conv1D(128, 5, activation='relu')(conv_1)\n",
    "pool = keras.layers.GlobalMaxPooling1D()(conv_2)\n",
    "flatten = keras.layers.Flatten()(pool)\n",
    "flatten_2 = keras.layers.Flatten()(emb_2)\n",
    "conc = keras.layers.Concatenate()([flatten,flatten_2, inp_num_data])\n",
    "Dense_1 = keras.layers.Dense(128, activation='relu')(conc)\n",
    "out = keras.layers.Dense(3, activation='sigmoid')(Dense_1)\n",
    "\n",
    "model = keras.Model(inputs=[inp_text_data,inp_cat_data, inp_num_data], outputs=out)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit([X_train_text,cat_train,num_train], y_train,\n",
    "                    epochs=7,\n",
    "                    validation_data=([X_test_text,cat_test,num_test], y_test),\n",
    "                    batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fa3aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d115358",
   "metadata": {},
   "source": [
    "### Model with regularization paramenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "54c17367",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embedding_second  = pd.get_dummies(df_full_agg, columns = ['annotatorGender','annotatorRace'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5f465a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = train_test_split(df_embedding_second, test_size=0.2, random_state=42, shuffle=True)\n",
    "y_train = to_categorical(train['offensiveYN'].values, 3)\n",
    "y_test = to_categorical(test['offensiveYN'].values, 3)\n",
    "\n",
    "text_train = train['clean_post'].values\n",
    "num_train = train['annotatorAge'].values\n",
    "cat_train = train.iloc[:,3:].to_numpy()\n",
    "\n",
    "text_test = test['clean_post'].values\n",
    "num_test = test['annotatorAge'].values\n",
    "cat_test = test.iloc[:,3:].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "50059524",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000) ### change \n",
    "tokenizer.fit_on_texts(text_train)\n",
    "\n",
    "X_train_text = tokenizer.texts_to_sequences(text_train)\n",
    "X_test_text = tokenizer.texts_to_sequences(text_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4129a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text = pad_sequences(X_train_text, padding='post', maxlen=maxlen)\n",
    "X_test_text = pad_sequences(X_test_text, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "27a02ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "embedding_matrix = create_embedding_matrix('glove.6B.50d.txt' ,\n",
    "                                            tokenizer.word_index,  \n",
    "                                            embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ff9b0882",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_embedding = min(np.ceil((cat_train.shape[1])/2), 50 ) ## determine categorical embedding size using conventional method\n",
    "cat_embedding_size = int(cat_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "76bbca46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "14155/14155 [==============================] - 508s 36ms/step - loss: 0.5179 - accuracy: 0.7579 - val_loss: 0.4648 - val_accuracy: 0.7856\n",
      "Epoch 2/10\n",
      "14155/14155 [==============================] - 500s 35ms/step - loss: 0.4399 - accuracy: 0.8047 - val_loss: 0.4696 - val_accuracy: 0.7869\n",
      "Epoch 3/10\n",
      "14155/14155 [==============================] - 501s 35ms/step - loss: 0.4006 - accuracy: 0.8285 - val_loss: 0.4482 - val_accuracy: 0.8007\n",
      "Epoch 4/10\n",
      "14155/14155 [==============================] - 491s 35ms/step - loss: 0.3712 - accuracy: 0.8430 - val_loss: 0.4570 - val_accuracy: 0.8002\n",
      "Epoch 5/10\n",
      "14155/14155 [==============================] - 493s 35ms/step - loss: 0.3471 - accuracy: 0.8560 - val_loss: 0.4748 - val_accuracy: 0.8052\n",
      "Epoch 6/10\n",
      "14155/14155 [==============================] - 489s 35ms/step - loss: 0.3277 - accuracy: 0.8664 - val_loss: 0.4884 - val_accuracy: 0.8035\n",
      "Epoch 7/10\n",
      "14155/14155 [==============================] - 489s 35ms/step - loss: 0.3137 - accuracy: 0.8720 - val_loss: 0.5015 - val_accuracy: 0.8043\n",
      "Epoch 8/10\n",
      "14155/14155 [==============================] - 494s 35ms/step - loss: 0.3012 - accuracy: 0.8772 - val_loss: 0.4770 - val_accuracy: 0.8073\n",
      "Epoch 9/10\n",
      "14155/14155 [==============================] - 499s 35ms/step - loss: 0.2894 - accuracy: 0.8800 - val_loss: 0.5039 - val_accuracy: 0.8063\n",
      "Epoch 10/10\n",
      "14155/14155 [==============================] - 502s 35ms/step - loss: 0.2800 - accuracy: 0.8839 - val_loss: 0.5274 - val_accuracy: 0.8064\n"
     ]
    }
   ],
   "source": [
    "inp_text_data = keras.layers.Input(shape=(X_train_text.shape[1],))\n",
    "inp_cat_data = keras.layers.Input(shape=(cat_train.shape[1],))\n",
    "inp_num_data = keras.layers.Input(shape=(1,))\n",
    "emb= keras.layers.Embedding(vocab_size, embedding_dim, input_length=maxlen)(inp_text_data)\n",
    "emb_2 = keras.layers.Embedding(input_dim=cat_train.shape[1], output_dim=cat_embedding_size)(inp_cat_data)\n",
    "conv_1 = keras.layers.Conv1D(128, 5, activation='relu',kernel_regularizer = regularizers.l1_l2(l1=1e-5,l2=1e-4)\n",
    "                             ,activity_regularizer = regularizers.l2(1e-5))(emb)\n",
    "conv_2 = keras.layers.Conv1D(128, 5, activation='relu')(conv_1)\n",
    "pool = keras.layers.GlobalMaxPooling1D()(conv_2)\n",
    "flatten = keras.layers.Flatten()(pool)\n",
    "flatten_2 = keras.layers.Flatten()(emb_2)\n",
    "conc = keras.layers.Concatenate()([flatten,flatten_2, inp_num_data])\n",
    "Dense_1 = keras.layers.Dense(128, activation='relu')(conc)\n",
    "dropout_1 = keras.layers.Dropout(0.2)(Dense_1)\n",
    "Dense_2 = keras.layers.Dense(128, activation='relu')(dropout_1)\n",
    "out = keras.layers.Dense(3, activation='sigmoid')(Dense_1)\n",
    "\n",
    "model = keras.Model(inputs=[inp_text_data,inp_cat_data, inp_num_data], outputs=out)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit([X_train_text,cat_train,num_train], y_train,\n",
    "                    epochs=10,\n",
    "                    validation_data=([X_test_text,cat_test,num_test], y_test),\n",
    "                    batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d899fd6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ade00a2e",
   "metadata": {},
   "source": [
    "### Model with separate embedding and  different max tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5024a5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embedding_second  = pd.get_dummies(df_full_agg, columns = ['annotatorGender','annotatorRace'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "09ad1749",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = train_test_split(df_embedding_second, test_size=0.2, random_state=42, shuffle=True)\n",
    "y_train = to_categorical(train['offensiveYN'].values, 3)\n",
    "y_test = to_categorical(test['offensiveYN'].values, 3)\n",
    "\n",
    "text_train = train['clean_post'].values\n",
    "num_train = train['annotatorAge'].values\n",
    "cat_train = train.iloc[:,3:].to_numpy()\n",
    "\n",
    "text_test = test['clean_post'].values\n",
    "num_test = test['annotatorAge'].values\n",
    "cat_test = test.iloc[:,3:].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5cb404ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=10000) ### change \n",
    "tokenizer.fit_on_texts(text_train)\n",
    "\n",
    "X_train_text = tokenizer.texts_to_sequences(text_train)\n",
    "X_test_text = tokenizer.texts_to_sequences(text_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "maxlen = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "83580c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text = pad_sequences(X_train_text, padding='post', maxlen=maxlen)\n",
    "X_test_text = pad_sequences(X_test_text, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a338a741",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "embedding_matrix = create_embedding_matrix('glove.6B.50d.txt' ,\n",
    "                                            tokenizer.word_index,  \n",
    "                                            embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b505e711",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_embedding = min(np.ceil((cat_train.shape[1])/2), 50 ) ## determine categorical embedding size using conventional method\n",
    "cat_embedding_size = int(cat_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f171fde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "14155/14155 [==============================] - 596s 42ms/step - loss: 0.5027 - accuracy: 0.7600 - val_loss: 0.4585 - val_accuracy: 0.7863\n",
      "Epoch 2/7\n",
      "14155/14155 [==============================] - 541s 38ms/step - loss: 0.3993 - accuracy: 0.8215 - val_loss: 0.4614 - val_accuracy: 0.8011\n",
      "Epoch 3/7\n",
      "14155/14155 [==============================] - 518s 37ms/step - loss: 0.3432 - accuracy: 0.8521 - val_loss: 0.4349 - val_accuracy: 0.8113\n",
      "Epoch 4/7\n",
      "14155/14155 [==============================] - 486s 34ms/step - loss: 0.3068 - accuracy: 0.8668 - val_loss: 0.4432 - val_accuracy: 0.8120\n",
      "Epoch 5/7\n",
      "14155/14155 [==============================] - 487s 34ms/step - loss: 0.2829 - accuracy: 0.8776 - val_loss: 0.4781 - val_accuracy: 0.8117\n",
      "Epoch 6/7\n",
      "14155/14155 [==============================] - 494s 35ms/step - loss: 0.2629 - accuracy: 0.8848 - val_loss: 0.4607 - val_accuracy: 0.8103\n",
      "Epoch 7/7\n",
      "14155/14155 [==============================] - 487s 34ms/step - loss: 0.2483 - accuracy: 0.8888 - val_loss: 0.4993 - val_accuracy: 0.8111\n"
     ]
    }
   ],
   "source": [
    "inp_text_data = keras.layers.Input(shape=(X_train_text.shape[1],))\n",
    "inp_cat_data = keras.layers.Input(shape=(cat_train.shape[1],))\n",
    "inp_num_data = keras.layers.Input(shape=(1,))\n",
    "emb= keras.layers.Embedding(vocab_size, embedding_dim, input_length=maxlen)(inp_text_data)\n",
    "emb_2 = keras.layers.Embedding(input_dim=cat_train.shape[1], output_dim=cat_embedding_size)(inp_cat_data)\n",
    "conv_1 = keras.layers.Conv1D(128, 5, activation='relu')(emb)\n",
    "conv_2 = keras.layers.Conv1D(128, 5, activation='relu')(conv_1)\n",
    "pool = keras.layers.GlobalMaxPooling1D()(conv_2)\n",
    "flatten = keras.layers.Flatten()(pool)\n",
    "flatten_2 = keras.layers.Flatten()(emb_2)\n",
    "conc = keras.layers.Concatenate()([flatten,flatten_2, inp_num_data])\n",
    "Dense_1 = keras.layers.Dense(128, activation='relu')(conc)\n",
    "out = keras.layers.Dense(3, activation='sigmoid')(Dense_1)\n",
    "\n",
    "model = keras.Model(inputs=[inp_text_data,inp_cat_data, inp_num_data], outputs=out)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit([X_train_text,cat_train,num_train], y_train,\n",
    "                    epochs=7,\n",
    "                    validation_data=([X_test_text,cat_test,num_test], y_test),\n",
    "                    batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bd50f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a10d7a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6271cbf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f52a19db",
   "metadata": {},
   "source": [
    "### Model with saperated embedding and removed stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d70b49a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hwu24/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13191d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d892208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
    "#text = pattern.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0326d4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_agg['clean_post'] = df_full_agg['clean_post'].map(lambda x: pattern.sub('', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be4c5d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nBill Kristol Ben Shaprio, two turds toilet bowl.\\n\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_agg.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e5bd8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embedding_second  = pd.get_dummies(df_full_agg, columns = ['annotatorGender','annotatorRace'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82983b3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_post</th>\n",
       "      <th>annotatorAge</th>\n",
       "      <th>offensiveYN</th>\n",
       "      <th>annotatorGender_man</th>\n",
       "      <th>annotatorGender_na</th>\n",
       "      <th>annotatorGender_nonBinary</th>\n",
       "      <th>annotatorGender_transman</th>\n",
       "      <th>annotatorGender_woman</th>\n",
       "      <th>annotatorRace_asian</th>\n",
       "      <th>annotatorRace_black</th>\n",
       "      <th>annotatorRace_hisp</th>\n",
       "      <th>annotatorRace_na</th>\n",
       "      <th>annotatorRace_native</th>\n",
       "      <th>annotatorRace_other</th>\n",
       "      <th>annotatorRace_white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nBill Kristol Ben Shaprio, two turds toilet...</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\nBill Kristol Ben Shaprio, two turds toilet...</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\nBill Kristol Ben Shaprio, two turds toilet...</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\nRose\\nüåπTaylor‚Äè @RealRoseTaylor 6h6 hours a...</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\nRose\\nüåπTaylor‚Äè @RealRoseTaylor 6h6 hours a...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_post  annotatorAge  \\\n",
       "0  \\n\\nBill Kristol Ben Shaprio, two turds toilet...          41.0   \n",
       "1  \\n\\nBill Kristol Ben Shaprio, two turds toilet...          42.0   \n",
       "2  \\n\\nBill Kristol Ben Shaprio, two turds toilet...          39.0   \n",
       "3  \\n\\nRose\\nüåπTaylor‚Äè @RealRoseTaylor 6h6 hours a...          25.0   \n",
       "4  \\n\\nRose\\nüåπTaylor‚Äè @RealRoseTaylor 6h6 hours a...          30.0   \n",
       "\n",
       "   offensiveYN  annotatorGender_man  annotatorGender_na  \\\n",
       "0          1.0                    1                   0   \n",
       "1          1.0                    1                   0   \n",
       "2          1.0                    0                   0   \n",
       "3          0.0                    1                   0   \n",
       "4          0.0                    0                   0   \n",
       "\n",
       "   annotatorGender_nonBinary  annotatorGender_transman  annotatorGender_woman  \\\n",
       "0                          0                         0                      0   \n",
       "1                          0                         0                      0   \n",
       "2                          0                         0                      1   \n",
       "3                          0                         0                      0   \n",
       "4                          0                         0                      1   \n",
       "\n",
       "   annotatorRace_asian  annotatorRace_black  annotatorRace_hisp  \\\n",
       "0                    0                    0                   0   \n",
       "1                    0                    0                   0   \n",
       "2                    0                    0                   0   \n",
       "3                    0                    0                   0   \n",
       "4                    0                    0                   0   \n",
       "\n",
       "   annotatorRace_na  annotatorRace_native  annotatorRace_other  \\\n",
       "0                 0                     0                    0   \n",
       "1                 0                     0                    0   \n",
       "2                 0                     0                    0   \n",
       "3                 0                     0                    0   \n",
       "4                 0                     0                    0   \n",
       "\n",
       "   annotatorRace_white  \n",
       "0                    1  \n",
       "1                    1  \n",
       "2                    1  \n",
       "3                    1  \n",
       "4                    1  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_embedding_second.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f03a25",
   "metadata": {},
   "source": [
    "### Prepare data for model consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9bedf2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = train_test_split(df_embedding_second, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00b905e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(train['offensiveYN'].values, 3)\n",
    "y_test = to_categorical(test['offensiveYN'].values, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05a941be",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = train['clean_post'].values\n",
    "num_train = train['annotatorAge'].values\n",
    "cat_train = train.iloc[:,3:].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd70674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000) ### change \n",
    "tokenizer.fit_on_texts(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "688bfe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test = test['clean_post'].values\n",
    "num_test = test['annotatorAge'].values\n",
    "cat_test = test.iloc[:,3:].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a56e17a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text = tokenizer.texts_to_sequences(text_train)\n",
    "X_test_text = tokenizer.texts_to_sequences(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ef03a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da6fe466",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text = pad_sequences(X_train_text, padding='post', maxlen=maxlen)\n",
    "X_test_text = pad_sequences(X_test_text, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f3a63829",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "embedding_matrix = create_embedding_matrix('glove.6B.50d.txt' ,\n",
    "                                            tokenizer.word_index,  \n",
    "                                            embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a33ea6ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70772, 100)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7ec7cb43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70772, 12)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "775980db",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_embedding = min(np.ceil((cat_train.shape[1])/2), 50 ) ## determine categorical embedding size using conventional method\n",
    "cat_embedding_size = int(cat_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b29cc54",
   "metadata": {},
   "source": [
    "### Feeding training data into model and monitor validation set performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e7bc426d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "14155/14155 [==============================] - 595s 42ms/step - loss: 0.5068 - accuracy: 0.7555 - val_loss: 0.4760 - val_accuracy: 0.7822\n",
      "Epoch 2/7\n",
      "14155/14155 [==============================] - 619s 44ms/step - loss: 0.4144 - accuracy: 0.8115 - val_loss: 0.4508 - val_accuracy: 0.7965\n",
      "Epoch 3/7\n",
      "14155/14155 [==============================] - 653s 46ms/step - loss: 0.3590 - accuracy: 0.8423 - val_loss: 0.4462 - val_accuracy: 0.7994\n",
      "Epoch 4/7\n",
      "14155/14155 [==============================] - 645s 46ms/step - loss: 0.3232 - accuracy: 0.8586 - val_loss: 0.4503 - val_accuracy: 0.8029\n",
      "Epoch 5/7\n",
      "14155/14155 [==============================] - 645s 46ms/step - loss: 0.2958 - accuracy: 0.8704 - val_loss: 0.4654 - val_accuracy: 0.8005\n",
      "Epoch 6/7\n",
      "14155/14155 [==============================] - 592s 42ms/step - loss: 0.2749 - accuracy: 0.8792 - val_loss: 0.5190 - val_accuracy: 0.8053\n",
      "Epoch 7/7\n",
      "14155/14155 [==============================] - 591s 42ms/step - loss: 0.2598 - accuracy: 0.8828 - val_loss: 0.4907 - val_accuracy: 0.8005\n"
     ]
    }
   ],
   "source": [
    "inp_text_data = keras.layers.Input(shape=(X_train_text.shape[1],))\n",
    "inp_cat_data = keras.layers.Input(shape=(cat_train.shape[1],))\n",
    "inp_num_data = keras.layers.Input(shape=(1,))\n",
    "emb= keras.layers.Embedding(vocab_size, embedding_dim, input_length=maxlen)(inp_text_data)\n",
    "emb_2 = keras.layers.Embedding(input_dim=cat_train.shape[1], output_dim=cat_embedding_size)(inp_cat_data)\n",
    "conv_1 = keras.layers.Conv1D(128, 5, activation='relu')(emb)\n",
    "conv_2 = keras.layers.Conv1D(128, 5, activation='relu')(conv_1)\n",
    "pool = keras.layers.GlobalMaxPooling1D()(conv_2)\n",
    "flatten = keras.layers.Flatten()(pool)\n",
    "flatten_2 = keras.layers.Flatten()(emb_2)\n",
    "conc = keras.layers.Concatenate()([flatten,flatten_2, inp_num_data])\n",
    "Dense_1 = keras.layers.Dense(128, activation='relu')(conc)\n",
    "out = keras.layers.Dense(3, activation='sigmoid')(Dense_1)\n",
    "\n",
    "model = keras.Model(inputs=[inp_text_data,inp_cat_data, inp_num_data], outputs=out)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit([X_train_text,cat_train,num_train], y_train,\n",
    "                    epochs=7,\n",
    "                    validation_data=([X_test_text,cat_test,num_test], y_test),\n",
    "                    batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf92462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df397bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dac36f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23316106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede5f507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8e7b74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
